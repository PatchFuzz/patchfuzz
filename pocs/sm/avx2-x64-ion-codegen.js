function codegenTestX64_v128xv128_v128_avxhack(inputs, options = {}) {
     for ( let [op, expected] of inputs ) {
         codegenTestX64_adhoc(wrap(options, `
         (func (export "f") (param v128 v128 v128) (result v128)
           (${op} (local.get 1) (local.get 2)))`),
                              'f',
                              expected,
                              options);
     }
}



function codegenTestX64_v128xLITERAL_v128_avxhack(inputs, options = {}) {
     for ( let [op, const_, expected] of inputs ) {
         codegenTestX64_adhoc(wrap(options, `
         (func (export "f") (param v128 v128) (result v128)
           (${op} (local.get 1) ${const_}))`),
                              'f',
                              expected,
                              options);
     }
}



function codegenTestX64_LITERALxv128_v128_avxhack(inputs, options = {}) {
     for ( let [op, const_, expected] of inputs ) {
         codegenTestX64_adhoc(wrap(options, `
         (func (export "f") (param v128 v128) (result v128)
           (${op} ${const_} (local.get 1)))`),
                              'f',
                              expected,
                              options);
     }
}




function codegenTestX64_T_v128_avxhack(inputs, options = {}) {
     for ( let [ty, op, expected] of inputs ) {
         codegenTestX64_adhoc(wrap(options, `
         (func (export "f") (param ${ty}) (result v128)
           (${op} (local.get 0)))`),
                              'f',
                              expected,
                              options);
     }
}


var GPR_I64 = "%r\\w+";
var GPR_I32 = "%(?:e\\w+|r\\d+d)";


codegenTestX64_v128xv128_v128_avxhack(
     [['i8x16.avgr_u',    `vpavgb %xmm2, %xmm1, %xmm0`],
      ['i16x8.avgr_u',    `vpavgw %xmm2, %xmm1, %xmm0`],
      ['i8x16.add',       `vpaddb %xmm2, %xmm1, %xmm0`],
      ['i8x16.add_sat_s', `vpaddsb %xmm2, %xmm1, %xmm0`],
      ['i8x16.add_sat_u', `vpaddusb %xmm2, %xmm1, %xmm0`],
      ['i8x16.sub',       `vpsubb %xmm2, %xmm1, %xmm0`],
      ['i8x16.sub_sat_s', `vpsubsb %xmm2, %xmm1, %xmm0`],
      ['i8x16.sub_sat_u', `vpsubusb %xmm2, %xmm1, %xmm0`],
      ['i16x8.mul',       `vpmullw %xmm2, %xmm1, %xmm0`],
      ['i16x8.min_s',     `vpminsw %xmm2, %xmm1, %xmm0`],
      ['i16x8.min_u',     `vpminuw %xmm2, %xmm1, %xmm0`],
      ['i16x8.max_s',     `vpmaxsw %xmm2, %xmm1, %xmm0`],
      ['i16x8.max_u',     `vpmaxuw %xmm2, %xmm1, %xmm0`],
      ['i32x4.add',       `vpaddd %xmm2, %xmm1, %xmm0`],
      ['i32x4.sub',       `vpsubd %xmm2, %xmm1, %xmm0`],
      ['i32x4.mul',       `vpmulld %xmm2, %xmm1, %xmm0`],
      ['i32x4.min_s',     `vpminsd %xmm2, %xmm1, %xmm0`],
      ['i32x4.min_u',     `vpminud %xmm2, %xmm1, %xmm0`],
      ['i32x4.max_s',     `vpmaxsd %xmm2, %xmm1, %xmm0`],
      ['i32x4.max_u',     `vpmaxud %xmm2, %xmm1, %xmm0`],
      ['i64x2.add',       `vpaddq %xmm2, %xmm1, %xmm0`],
      ['i64x2.sub',       `vpsubq %xmm2, %xmm1, %xmm0`],
      ['i64x2.mul', `
vpsrlq \\$0x20, %xmm1, %xmm3
pmuludq %xmm2, %xmm3
vpsrlq \\$0x20, %xmm2, %xmm15
pmuludq %xmm1, %xmm15
paddq %xmm3, %xmm15
psllq \\$0x20, %xmm15
vpmuludq %xmm2, %xmm1, %xmm0
paddq %xmm15, %xmm0`],
      ['f32x4.add',            `vaddps %xmm2, %xmm1, %xmm0`],
      ['f32x4.sub',            `vsubps %xmm2, %xmm1, %xmm0`],
      ['f32x4.mul',            `vmulps %xmm2, %xmm1, %xmm0`],
      ['f32x4.div',            `vdivps %xmm2, %xmm1, %xmm0`],
      ['f64x2.add',            `vaddpd %xmm2, %xmm1, %xmm0`],
      ['f64x2.sub',            `vsubpd %xmm2, %xmm1, %xmm0`],
      ['f64x2.mul',            `vmulpd %xmm2, %xmm1, %xmm0`],
      ['f64x2.div',            `vdivpd %xmm2, %xmm1, %xmm0`],
      ['i8x16.narrow_i16x8_s', `vpacksswb %xmm2, %xmm1, %xmm0`],
      ['i8x16.narrow_i16x8_u', `vpackuswb %xmm2, %xmm1, %xmm0`],
      ['i16x8.narrow_i32x4_s', `vpackssdw %xmm2, %xmm1, %xmm0`],
      ['i16x8.narrow_i32x4_u', `vpackusdw %xmm2, %xmm1, %xmm0`],
      ['i32x4.dot_i16x8_s',    `vpmaddwd %xmm2, %xmm1, %xmm0`]]);


codegenTestX64_v128xv128_v128_avxhack(
     [['i8x16.eq', `vpcmpeqb %xmm2, %xmm1, %xmm0`],
      ['i8x16.ne', `
vpcmpeqb %xmm2, %xmm1, %xmm0
pcmpeqw %xmm15, %xmm15
pxor %xmm15, %xmm0`],
      ['i8x16.lt_s', `vpcmpgtb %xmm1, %xmm2, %xmm0`],
      ['i8x16.gt_u', `
vpmaxub %xmm2, %xmm1, %xmm0
pcmpeqb %xmm2, %xmm0
pcmpeqw %xmm15, %xmm15
pxor %xmm15, %xmm0`],
      ['i16x8.eq', `vpcmpeqw %xmm2, %xmm1, %xmm0`],
      ['i16x8.ne', `
vpcmpeqw %xmm2, %xmm1, %xmm0
pcmpeqw %xmm15, %xmm15
pxor %xmm15, %xmm0`],
      ['i16x8.le_s', `
vpcmpgtw %xmm2, %xmm1, %xmm0
pcmpeqw %xmm15, %xmm15
pxor %xmm15, %xmm0`],
      ['i16x8.ge_u', `
vpminuw %xmm2, %xmm1, %xmm0
pcmpeqw %xmm2, %xmm0`],
      ['i32x4.eq', `vpcmpeqd %xmm2, %xmm1, %xmm0`],
      ['i32x4.ne', `
vpcmpeqd %xmm2, %xmm1, %xmm0
pcmpeqw %xmm15, %xmm15
pxor %xmm15, %xmm0`],
      ['i32x4.lt_s', `vpcmpgtd %xmm1, %xmm2, %xmm0`],
      ['i32x4.gt_u', `
vpmaxud %xmm2, %xmm1, %xmm0
pcmpeqd %xmm2, %xmm0
pcmpeqw %xmm15, %xmm15
pxor %xmm15, %xmm0`],
      ['i64x2.eq', `vpcmpeqq %xmm2, %xmm1, %xmm0`],
      ['i64x2.ne', `
vpcmpeqq %xmm2, %xmm1, %xmm0
pcmpeqw %xmm15, %xmm15
pxor %xmm15, %xmm0`],
      ['i64x2.lt_s', `vpcmpgtq %xmm1, %xmm2, %xmm0`],
      ['i64x2.ge_s', `
vpcmpgtq %xmm1, %xmm2, %xmm0
pcmpeqw %xmm15, %xmm15
pxor %xmm15, %xmm0`],
      ['f32x4.eq', `vcmpps \\$0x00, %xmm2, %xmm1, %xmm0`],
      ['f32x4.lt', `vcmpps \\$0x01, %xmm2, %xmm1, %xmm0`],
      ['f32x4.ge', `vcmpps \\$0x02, %xmm1, %xmm2, %xmm0`],
      ['f64x2.eq', `vcmppd \\$0x00, %xmm2, %xmm1, %xmm0`],
      ['f64x2.lt', `vcmppd \\$0x01, %xmm2, %xmm1, %xmm0`],
      ['f64x2.ge', `vcmppd \\$0x02, %xmm1, %xmm2, %xmm0`],
      ['f32x4.pmin', `vminps %xmm1, %xmm2, %xmm0`],
      ['f32x4.pmax', `vmaxps %xmm1, %xmm2, %xmm0`],
      ['f64x2.pmin', `vminpd %xmm1, %xmm2, %xmm0`],
      ['f64x2.pmax', `vmaxpd %xmm1, %xmm2, %xmm0`],
      ['i8x16.swizzle', `
vpaddusbx ${RIPR}, %xmm2, %xmm15
vpshufb %xmm15, %xmm1, %xmm0`],
      ['i16x8.extmul_high_i8x16_s', `
palignr \\$0x08, %xmm2, %xmm15
vpmovsxbw %xmm15, %xmm15
palignr \\$0x08, %xmm1, %xmm0
vpmovsxbw %xmm0, %xmm0
pmullw %xmm15, %xmm0`],
      ['i32x4.extmul_low_i16x8_u', `
vpmulhuw %xmm2, %xmm1, %xmm15
vpmullw %xmm2, %xmm1, %xmm0
punpcklwd %xmm15, %xmm0`],
      ['i64x2.extmul_low_i32x4_s', `
vpshufd \\$0x10, %xmm1, %xmm15
vpshufd \\$0x10, %xmm2, %xmm0
pmuldq %xmm15, %xmm0`],
      ['i16x8.q15mulr_sat_s', `
vpmulhrsw %xmm2, %xmm1, %xmm0
vpcmpeqwx ${RIPR}, %xmm0, %xmm15
pxor %xmm15, %xmm0`],
]);


codegenTestX64_v128xv128_v128_avxhack(
     [['v128.and', `vpand %xmm2, %xmm1, %xmm0`],
      ['v128.andnot', `vpandn %xmm1, %xmm2, %xmm0`],
      ['v128.or', `vpor %xmm2, %xmm1, %xmm0`],
      ['v128.xor', `vpxor %xmm2, %xmm1, %xmm0`]]);



codegenTestX64_adhoc(`(module
     (func (export "f") (param v128 v128 i32) (result v128)
          (i8x16.replace_lane 7 (local.get 1) (local.get 2))))`, 'f', `
vpinsrb \\$0x07, ${GPR_I32}, %xmm1, %xmm0`);
codegenTestX64_adhoc(`(module
     (func (export "f") (param v128 v128 i32) (result v128)
          (i16x8.replace_lane 3 (local.get 1) (local.get 2))))`, 'f', `
vpinsrw \\$0x03, ${GPR_I32}, %xmm1, %xmm0`);
codegenTestX64_adhoc(`(module
     (func (export "f") (param v128 v128 i32) (result v128)
          (i32x4.replace_lane 2 (local.get 1) (local.get 2))))`, 'f', `
vpinsrd \\$0x02, ${GPR_I32}, %xmm1, %xmm0`);
codegenTestX64_adhoc(`(module
     (func (export "f") (param v128 v128 i64) (result v128)
          (i64x2.replace_lane 1 (local.get 1) (local.get 2))))`, 'f', `
vpinsrq \\$0x01, ${GPR_I64}, %xmm1, %xmm0`);


if (isAvxPresent(2)) {
     codegenTestX64_T_v128_avxhack(
          [['i32', 'i8x16.splat', `
vmovd ${GPR_I32}, %xmm0
vpbroadcastb %xmm0, %xmm0`],
           ['i32', 'i16x8.splat', `
vmovd ${GPR_I32}, %xmm0
vpbroadcastw %xmm0, %xmm0`],
           ['i32', 'i32x4.splat', `
vmovd ${GPR_I32}, %xmm0
vpbroadcastd %xmm0, %xmm0`],
           ['i64', 'i64x2.splat', `
vmovq ${GPR_I64}, %xmm0
vpbroadcastq %xmm0, %xmm0`],
           ['f32', 'f32x4.splat', `vbroadcastss %xmm0, %xmm0`]], {log:true});

     codegenTestX64_T_v128_avxhack(
          [['i32', 'v128.load8_splat',
            'vpbroadcastbb \\(%r15,%r\\w+,1\\), %xmm0'],
           ['i32', 'v128.load16_splat',
            'vpbroadcastww \\(%r15,%r\\w+,1\\), %xmm0'],
           ['i32', 'v128.load32_splat',
            'vbroadcastssl \\(%r15,%r\\w+,1\\), %xmm0']], {memory: 1});
}


codegenTestX64_v128xv128_v128_avxhack([
     
    ['i8x16.shuffle 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15',
     'vmovdqa %xmm1, %xmm0'],

     
    ['i8x16.shuffle 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5',
     `
vpunpcklbw %xmm1, %xmm1, %xmm0
vpshufhw \\$0x55, %xmm0, %xmm0
vpshufd \\$0xAA, %xmm0, %xmm0`],

     
    ['i8x16.shuffle 4 5 4 5 4 5 4 5 4 5 4 5 4 5 4 5',
     `
vpshuflw \\$0xAA, %xmm1, %xmm0
vpshufd \\$0x00, %xmm0, %xmm0`],

     
     ['i8x16.shuffle 2 3 0 1 6 7 4 5 10 11 8 9 14 15 12 13',
`
vpshuflw \\$0xB1, %xmm1, %xmm0
vpshufhw \\$0xB1, %xmm0, %xmm0`],

     
     ['i8x16.shuffle 4 5 6 7 0 1 2 3 12 13 14 15 8 9 10 11',
      'vpshufd \\$0xB1, %xmm1, %xmm0'],

     
     ['i8x16.shuffle 0 1 2 3 16 17 18 19 4 5 6 7 20 21 22 23',
      'vpunpckldq %xmm2, %xmm1, %xmm0'],

     
     ['i8x16.shuffle 24 25 26 27 28 29 30 31 8 9 10 11 12 13 14 15',
      'vpunpckhqdq %xmm1, %xmm2, %xmm0'],

     
    ['i8x16.shuffle 13 14 15 0 1 2 3 4 5 6 7 8 9 10 11 12',
     `vpalignr \\$0x0D, %xmm1, %xmm1, %xmm0`],
    ['i8x16.shuffle 28 29 30 31 0 1 2 3 4 5 6 7 8 9 10 11',
     `vpalignr \\$0x0C, %xmm2, %xmm1, %xmm0`]]);

if (isAvxPresent(2)) {
     codegenTestX64_v128xv128_v128_avxhack([
          
          ['i8x16.shuffle 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0',
           'vpbroadcastb %xmm1, %xmm0'],

          
          ['i8x16.shuffle 16 17 16 17 16 17 16 17 16 17 16 17 16 17 16 17',
          'vpbroadcastw %xmm2, %xmm0'],

          
          ['i8x16.shuffle 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3',
           'vpbroadcastd %xmm1, %xmm0']]);
}


codegenTestX64_adhoc(
     `(func (export "f") (param v128 v128 v128 v128) (result v128)
        (i8x16.shuffle 0 17 2 3 4 5 6 7 24 25 26 11 12 13 30 15
          (local.get 2)(local.get 3)))`,
     'f',
`
movdqax ${RIPR}, %xmm1
vpblendvb %xmm1, %xmm3, %xmm2, %xmm0`);


codegenTestX64_v128xLITERAL_v128_avxhack(
     [['i8x16.add', '(v128.const i8x16 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2)',
       `vpaddbx ${RIPR}, %xmm1, %xmm0`],
      ['i8x16.sub', '(v128.const i8x16 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2)',
       `vpsubbx ${RIPR}, %xmm1, %xmm0`],
      ['i8x16.add_sat_s', '(v128.const i8x16 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2)',
       `vpaddsbx ${RIPR}, %xmm1, %xmm0`],
      ['i8x16.add_sat_u', '(v128.const i8x16 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2)',
       `vpaddusbx ${RIPR}, %xmm1, %xmm0`],
      ['i8x16.sub_sat_s', '(v128.const i8x16 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2)',
       `vpsubsbx ${RIPR}, %xmm1, %xmm0`],
      ['i8x16.sub_sat_u', '(v128.const i8x16 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2)',
       `vpsubusbx ${RIPR}, %xmm1, %xmm0`],
      ['i8x16.min_s', '(v128.const i8x16 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2)',
       `vpminsbx ${RIPR}, %xmm1, %xmm0`],
      ['i8x16.min_u', '(v128.const i8x16 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2)',
       `vpminubx ${RIPR}, %xmm1, %xmm0`],
      ['i8x16.max_s', '(v128.const i8x16 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2)',
       `vpmaxsbx ${RIPR}, %xmm1, %xmm0`],
      ['i8x16.max_u', '(v128.const i8x16 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2)',
       `vpmaxubx ${RIPR}, %xmm1, %xmm0`],
      ['i8x16.eq', '(v128.const i8x16 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2)',
       `vpcmpeqbx ${RIPR}, %xmm1, %xmm0`],
      ['i8x16.ne', '(v128.const i8x16 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2)', `
 vpcmpeqbx ${RIPR}, %xmm1, %xmm0
 pcmpeqw %xmm15, %xmm15
 pxor %xmm15, %xmm0`],
      ['i8x16.gt_s', '(v128.const i8x16 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2)',
       `vpcmpgtbx ${RIPR}, %xmm1, %xmm0`],
      ['i8x16.le_s', '(v128.const i8x16 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2)', `
 vpcmpgtbx ${RIPR}, %xmm1, %xmm0
 pcmpeqw %xmm15, %xmm15
 pxor %xmm15, %xmm0`],
      ['i8x16.narrow_i16x8_s', '(v128.const i8x16 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2)',
       `vpacksswbx ${RIPR}, %xmm1, %xmm0`],
      ['i8x16.narrow_i16x8_u', '(v128.const i8x16 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2)',
       `vpackuswbx ${RIPR}, %xmm1, %xmm0`],

      ['i16x8.add', '(v128.const i16x8 1 2 1 2 1 2 1 2)',
       `vpaddwx ${RIPR}, %xmm1, %xmm0`],
      ['i16x8.sub', '(v128.const i16x8 1 2 1 2 1 2 1 2)',
       `vpsubwx ${RIPR}, %xmm1, %xmm0`],
      ['i16x8.mul', '(v128.const i16x8 1 2 1 2 1 2 1 2)',
       `vpmullwx ${RIPR}, %xmm1, %xmm0`],
      ['i16x8.add_sat_s', '(v128.const i16x8 1 2 1 2 1 2 1 2)',
       `vpaddswx ${RIPR}, %xmm1, %xmm0`],
      ['i16x8.add_sat_u', '(v128.const i16x8 1 2 1 2 1 2 1 2)',
       `vpadduswx ${RIPR}, %xmm1, %xmm0`],
      ['i16x8.sub_sat_s', '(v128.const i16x8 1 2 1 2 1 2 1 2)',
       `vpsubswx ${RIPR}, %xmm1, %xmm0`],
      ['i16x8.sub_sat_u', '(v128.const i16x8 1 2 1 2 1 2 1 2)',
       `vpsubuswx ${RIPR}, %xmm1, %xmm0`],
      ['i16x8.min_s', '(v128.const i16x8 1 2 1 2 1 2 1 2)',
       `vpminswx ${RIPR}, %xmm1, %xmm0`],
      ['i16x8.min_u', '(v128.const i16x8 1 2 1 2 1 2 1 2)',
       `vpminuwx ${RIPR}, %xmm1, %xmm0`],
      ['i16x8.max_s', '(v128.const i16x8 1 2 1 2 1 2 1 2)',
       `vpmaxswx ${RIPR}, %xmm1, %xmm0`],
      ['i16x8.max_u', '(v128.const i16x8 1 2 1 2 1 2 1 2)',
       `vpmaxuwx ${RIPR}, %xmm1, %xmm0`],
      ['i16x8.eq', '(v128.const i16x8 1 2 1 2 1 2 1 2)',
       `vpcmpeqwx ${RIPR}, %xmm1, %xmm0`],
      ['i16x8.ne', '(v128.const i16x8 1 2 1 2 1 2 1 2)', `
 vpcmpeqwx ${RIPR}, %xmm1, %xmm0
 pcmpeqw %xmm15, %xmm15
 pxor %xmm15, %xmm0`],
      ['i16x8.gt_s', '(v128.const i16x8 1 2 1 2 1 2 1 2)',
       `vpcmpgtwx ${RIPR}, %xmm1, %xmm0`],
      ['i16x8.le_s', '(v128.const i16x8 1 2 1 2 1 2 1 2)', `
 vpcmpgtwx ${RIPR}, %xmm1, %xmm0
 pcmpeqw %xmm15, %xmm15
 pxor %xmm15, %xmm0`],
      ['i16x8.narrow_i32x4_s', '(v128.const i16x8 1 2 1 2 1 2 1 2)',
       `vpackssdwx ${RIPR}, %xmm1, %xmm0`],
      ['i16x8.narrow_i32x4_u', '(v128.const i16x8 1 2 1 2 1 2 1 2)',
       `vpackusdwx ${RIPR}, %xmm1, %xmm0`],

      ['i32x4.add', '(v128.const i32x4 1 2 1 2)',
       `vpadddx ${RIPR}, %xmm1, %xmm0`],
      ['i32x4.sub', '(v128.const i32x4 1 2 1 2)',
       `vpsubdx ${RIPR}, %xmm1, %xmm0`],
      ['i32x4.mul', '(v128.const i32x4 1 2 1 2)',
       `vpmulldx ${RIPR}, %xmm1, %xmm0`],
      ['i32x4.min_s', '(v128.const i32x4 1 2 1 2)',
       `vpminsdx ${RIPR}, %xmm1, %xmm0`],
      ['i32x4.min_u', '(v128.const i32x4 1 2 1 2)',
       `vpminudx ${RIPR}, %xmm1, %xmm0`],
      ['i32x4.max_s', '(v128.const i32x4 1 2 1 2)',
       `vpmaxsdx ${RIPR}, %xmm1, %xmm0`],
      ['i32x4.max_u', '(v128.const i32x4 1 2 1 2)',
       `vpmaxudx ${RIPR}, %xmm1, %xmm0`],
      ['i32x4.eq', '(v128.const i32x4 1 2 1 2)',
       `vpcmpeqdx ${RIPR}, %xmm1, %xmm0`],
      ['i32x4.ne', '(v128.const i32x4 1 2 1 2)', `
 vpcmpeqdx ${RIPR}, %xmm1, %xmm0
 pcmpeqw %xmm15, %xmm15
 pxor %xmm15, %xmm0`],
      ['i32x4.gt_s', '(v128.const i32x4 1 2 1 2)',
       `vpcmpgtdx ${RIPR}, %xmm1, %xmm0`],
      ['i32x4.le_s', '(v128.const i32x4 1 2 1 2)', `
 vpcmpgtdx ${RIPR}, %xmm1, %xmm0
 pcmpeqw %xmm15, %xmm15
 pxor %xmm15, %xmm0`],
      ['i32x4.dot_i16x8_s', '(v128.const i32x4 1 2 1 2)',
       `vpmaddwdx ${RIPR}, %xmm1, %xmm0`],

      ['i64x2.add', '(v128.const i64x2 1 2)',
       `vpaddqx ${RIPR}, %xmm1, %xmm0`],
      ['i64x2.sub', '(v128.const i64x2 1 2)',
       `vpsubqx ${RIPR}, %xmm1, %xmm0`],

      ['v128.and', '(v128.const i8x16 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2)',
       `vpandx ${RIPR}, %xmm1, %xmm0`],
      ['v128.or', '(v128.const i8x16 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2)',
       `vporx ${RIPR}, %xmm1, %xmm0`],
      ['v128.xor', '(v128.const i8x16 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2)',
       `vpxorx ${RIPR}, %xmm1, %xmm0`],

      ['f32x4.add', '(v128.const f32x4 1 2 3 4)',
       `vaddpsx ${RIPR}, %xmm1, %xmm0`],
      ['f32x4.sub', '(v128.const f32x4 1 2 3 4)',
       `vsubpsx ${RIPR}, %xmm1, %xmm0`],
      ['f32x4.mul', '(v128.const f32x4 1 2 3 4)',
       `vmulpsx ${RIPR}, %xmm1, %xmm0`],
      ['f32x4.div', '(v128.const f32x4 1 2 3 4)',
       `vdivpsx ${RIPR}, %xmm1, %xmm0`],

      ['f64x2.add', '(v128.const f64x2 1 2)',
       `vaddpdx ${RIPR}, %xmm1, %xmm0`],
      ['f64x2.sub', '(v128.const f64x2 1 2)',
       `vsubpdx ${RIPR}, %xmm1, %xmm0`],
      ['f64x2.mul', '(v128.const f64x2 1 2)',
       `vmulpdx ${RIPR}, %xmm1, %xmm0`],
      ['f64x2.div', '(v128.const f64x2 1 2)',
       `vdivpdx ${RIPR}, %xmm1, %xmm0`],

      ['f32x4.eq', '(v128.const f32x4 1 2 3 4)',
       `vcmppsx \\$0x00, ${RIPR}, %xmm1, %xmm0`],
      ['f32x4.ne', '(v128.const f32x4 1 2 3 4)',
       `vcmppsx \\$0x04, ${RIPR}, %xmm1, %xmm0`],
      ['f32x4.lt', '(v128.const f32x4 1 2 3 4)',
       `vcmppsx \\$0x01, ${RIPR}, %xmm1, %xmm0`],
      ['f32x4.le', '(v128.const f32x4 1 2 3 4)',
       `vcmppsx \\$0x02, ${RIPR}, %xmm1, %xmm0`],

      ['f64x2.eq', '(v128.const f64x2 1 2)',
       `vcmppdx \\$0x00, ${RIPR}, %xmm1, %xmm0`],
      ['f64x2.ne', '(v128.const f64x2 1 2)',
       `vcmppdx \\$0x04, ${RIPR}, %xmm1, %xmm0`],
      ['f64x2.lt', '(v128.const f64x2 1 2)',
       `vcmppdx \\$0x01, ${RIPR}, %xmm1, %xmm0`],
      ['f64x2.le', '(v128.const f64x2 1 2)',
       `vcmppdx \\$0x02, ${RIPR}, %xmm1, %xmm0`]]);

 
 
 codegenTestX64_LITERALxv128_v128_avxhack(
     [['i8x16.add', '(v128.const i8x16 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2)',
       `vpaddbx ${RIPR}, %xmm1, %xmm0`],
      ['i8x16.add_sat_s', '(v128.const i8x16 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2)',
       `vpaddsbx ${RIPR}, %xmm1, %xmm0`],
      ['i8x16.add_sat_u', '(v128.const i8x16 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2)',
       `vpaddusbx ${RIPR}, %xmm1, %xmm0`],
      ['i8x16.min_s', '(v128.const i8x16 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2)',
       `vpminsbx ${RIPR}, %xmm1, %xmm0`],
      ['i8x16.min_u', '(v128.const i8x16 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2)',
       `vpminubx ${RIPR}, %xmm1, %xmm0`],
      ['i8x16.max_s', '(v128.const i8x16 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2)',
       `vpmaxsbx ${RIPR}, %xmm1, %xmm0`],
      ['i8x16.max_u', '(v128.const i8x16 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2)',
       `vpmaxubx ${RIPR}, %xmm1, %xmm0`],
      ['i8x16.eq', '(v128.const i8x16 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2)',
       `vpcmpeqbx ${RIPR}, %xmm1, %xmm0`],
      ['i8x16.ne', '(v128.const i8x16 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2)', `
 vpcmpeqbx ${RIPR}, %xmm1, %xmm0
 pcmpeqw %xmm15, %xmm15
 pxor %xmm15, %xmm0`],

      ['i16x8.add', '(v128.const i16x8 1 2 1 2 1 2 1 2)',
       `vpaddwx ${RIPR}, %xmm1, %xmm0`],
      ['i16x8.mul', '(v128.const i16x8 1 2 1 2 1 2 1 2)',
       `vpmullwx ${RIPR}, %xmm1, %xmm0`],
      ['i16x8.add_sat_s', '(v128.const i16x8 1 2 1 2 1 2 1 2)',
       `vpaddswx ${RIPR}, %xmm1, %xmm0`],
      ['i16x8.add_sat_u', '(v128.const i16x8 1 2 1 2 1 2 1 2)',
       `vpadduswx ${RIPR}, %xmm1, %xmm0`],
      ['i16x8.min_s', '(v128.const i16x8 1 2 1 2 1 2 1 2)',
       `vpminswx ${RIPR}, %xmm1, %xmm0`],
      ['i16x8.min_u', '(v128.const i16x8 1 2 1 2 1 2 1 2)',
       `vpminuwx ${RIPR}, %xmm1, %xmm0`],
      ['i16x8.max_s', '(v128.const i16x8 1 2 1 2 1 2 1 2)',
       `vpmaxswx ${RIPR}, %xmm1, %xmm0`],
      ['i16x8.max_u', '(v128.const i16x8 1 2 1 2 1 2 1 2)',
       `vpmaxuwx ${RIPR}, %xmm1, %xmm0`],
      ['i16x8.eq', '(v128.const i16x8 1 2 1 2 1 2 1 2)',
       `vpcmpeqwx ${RIPR}, %xmm1, %xmm0`],
      ['i16x8.ne', '(v128.const i16x8 1 2 1 2 1 2 1 2)', `
 vpcmpeqwx ${RIPR}, %xmm1, %xmm0
 pcmpeqw %xmm15, %xmm15
 pxor %xmm15, %xmm0`],

      ['i32x4.add', '(v128.const i32x4 1 2 1 2)',
       `vpadddx ${RIPR}, %xmm1, %xmm0`],
      ['i32x4.mul', '(v128.const i32x4 1 2 1 2)',
       `vpmulldx ${RIPR}, %xmm1, %xmm0`],
      ['i32x4.min_s', '(v128.const i32x4 1 2 1 2)',
       `vpminsdx ${RIPR}, %xmm1, %xmm0`],
      ['i32x4.min_u', '(v128.const i32x4 1 2 1 2)',
       `vpminudx ${RIPR}, %xmm1, %xmm0`],
      ['i32x4.max_s', '(v128.const i32x4 1 2 1 2)',
       `vpmaxsdx ${RIPR}, %xmm1, %xmm0`],
      ['i32x4.max_u', '(v128.const i32x4 1 2 1 2)',
       `vpmaxudx ${RIPR}, %xmm1, %xmm0`],
      ['i32x4.eq', '(v128.const i32x4 1 2 1 2)',
       `vpcmpeqdx ${RIPR}, %xmm1, %xmm0`],
      ['i32x4.ne', '(v128.const i32x4 1 2 1 2)', `
 vpcmpeqdx ${RIPR}, %xmm1, %xmm0
 pcmpeqw %xmm15, %xmm15
 pxor %xmm15, %xmm0`],
      ['i32x4.dot_i16x8_s', '(v128.const i32x4 1 2 1 2)',
       `vpmaddwdx ${RIPR}, %xmm1, %xmm0`],

      ['i64x2.add', '(v128.const i64x2 1 2)',
       `vpaddqx ${RIPR}, %xmm1, %xmm0`],

      ['v128.and', '(v128.const i8x16 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2)',
       `vpandx ${RIPR}, %xmm1, %xmm0`],
      ['v128.or', '(v128.const i8x16 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2)',
       `vporx ${RIPR}, %xmm1, %xmm0`],
      ['v128.xor', '(v128.const i8x16 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2)',
       `vpxorx ${RIPR}, %xmm1, %xmm0`]]);


codegenTestX64_v128xLITERAL_v128_avxhack(
     [['i8x16.shl', '(i32.const 2)', `
vpaddb %xmm1, %xmm1, %xmm0
paddb %xmm0, %xmm0`],
      ['i8x16.shl', '(i32.const 4)', `
vpandx ${RIPR}, %xmm1, %xmm0
psllw \\$0x04, %xmm0`],
      ['i16x8.shl', '(i32.const 1)',
       'vpsllw \\$0x01, %xmm1, %xmm0'],
      ['i16x8.shr_s', '(i32.const 3)',
       'vpsraw \\$0x03, %xmm1, %xmm0'],
      ['i16x8.shr_u', '(i32.const 2)',
       'vpsrlw \\$0x02, %xmm1, %xmm0'],
      ['i32x4.shl', '(i32.const 5)',
       'vpslld \\$0x05, %xmm1, %xmm0'],
      ['i32x4.shr_s', '(i32.const 2)',
       'vpsrad \\$0x02, %xmm1, %xmm0'],
      ['i32x4.shr_u', '(i32.const 5)',
       'vpsrld \\$0x05, %xmm1, %xmm0'],
      ['i64x2.shr_s', '(i32.const 7)', `
vpshufd \\$0xF5, %xmm1, %xmm15
psrad \\$0x1F, %xmm15
vpxor %xmm15, %xmm1, %xmm0
psrlq \\$0x07, %xmm0
pxor %xmm15, %xmm0`]]);


codegenTestX64_adhoc(
     `(module
         (func (export "f") (param v128) (param v128) (param v128) (param v128) (result v128)
           (v128.bitselect (local.get 2) (local.get 3)
              (i32x4.eq (local.get 0) (local.get 1)))))`,
         'f', `
pcmpeqd %xmm1, %xmm0
vpblendvb %xmm0, %xmm2, %xmm3, %xmm0`);
